{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mamba-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mamba_ssm.modules.mamba_simple import Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch, length, dim = 2, 64, 16\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "model = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=16,  # SSM state expansion factor\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mamba(\n",
       "  (in_proj): Linear(in_features=16, out_features=64, bias=False)\n",
       "  (conv1d): Conv1d(32, 32, kernel_size=(4,), stride=(1,), padding=(3,), groups=32)\n",
       "  (act): SiLU()\n",
       "  (x_proj): Linear(in_features=32, out_features=33, bias=False)\n",
       "  (dt_proj): Linear(in_features=1, out_features=32, bias=True)\n",
       "  (out_proj): Linear(in_features=32, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hey how are you doing?\\n\\nI'm so glad you're here.\"]\n"
     ]
    }
   ],
   "source": [
    "# load pretrained mamba model\n",
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "input_ids = tokenizer(\"Hey how are you doing?\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "out = model.generate(input_ids, max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey how are you doing?\\n\\nI'm so glad you're here.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_output(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    out = model.generate(input_ids, max_new_tokens=10)\n",
    "    return tokenizer.batch_decode(out)[0]\n",
    "\n",
    "get_output(\"Hey how are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1=2$ and $1+1=3$'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_output(\"1+1=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generation_time = end_time - start_time\n",
    "    \n",
    "    return generated_text, generation_time\n",
    "\n",
    "def main():\n",
    "    prompt = \"Once upon a time, in a land far away,\"\n",
    "    max_length = 100\n",
    "\n",
    "    # Mamba 130M model\n",
    "    mamba_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "    mamba_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "\n",
    "    # Pythia-160M model\n",
    "    pythia_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\")\n",
    "    pythia_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")\n",
    "\n",
    "    print(\"Generating text with Mamba 130M...\")\n",
    "    mamba_text, mamba_time = generate_text(mamba_model, mamba_tokenizer, prompt, max_length)\n",
    "    \n",
    "    print(\"Generating text with Pythia-160M...\")\n",
    "    pythia_text, pythia_time = generate_text(pythia_model, pythia_tokenizer, prompt, max_length)\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Mamba 130M generation time: {mamba_time:.4f} seconds\")\n",
    "    print(f\"Pythia-160M generation time: {pythia_time:.4f} seconds\")\n",
    "    print(f\"\\nMamba 130M generated text:\\n{mamba_text}, {len(mamba_text)}\")\n",
    "    print(f\"\\nPythia-160M generated text:\\n{pythia_text}, {len(pythia_text)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[111 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.5.0+cu124\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m Guessing wheel URL:  https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.4.0/causal_conv1d-1.4.0+cu122torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n",
      "  \u001b[31m   \u001b[0m Precompiled wheel not found. Building from source...\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/causal_conv1d\n",
      "  \u001b[31m   \u001b[0m copying causal_conv1d/__init__.py -> build/lib.linux-x86_64-cpython-310/causal_conv1d\n",
      "  \u001b[31m   \u001b[0m copying causal_conv1d/causal_conv1d_interface.py -> build/lib.linux-x86_64-cpython-310/causal_conv1d\n",
      "  \u001b[31m   \u001b[0m copying causal_conv1d/causal_conv1d_varlen.py -> build/lib.linux-x86_64-cpython-310/causal_conv1d\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m /opt/conda/envs/mamba-env/lib/python3.10/site-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.1) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
      "  \u001b[31m   \u001b[0m /opt/conda/envs/mamba-env/lib/python3.10/site-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no g++ version bounds defined for CUDA version 12.1\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
      "  \u001b[31m   \u001b[0m building 'causal_conv1d_cuda' extension\n",
      "  \u001b[31m   \u001b[0m creating /tmp/pip-install-ruf20zbg/causal-conv1d_6adcc87d8b934d4ab1c1f004d6c48513/build/temp.linux-x86_64-cpython-310/csrc\n",
      "  \u001b[31m   \u001b[0m Emitting ninja build file /tmp/pip-install-ruf20zbg/causal-conv1d_6adcc87d8b934d4ab1c1f004d6c48513/build/temp.linux-x86_64-cpython-310/build.ninja...\n",
      "  \u001b[31m   \u001b[0m Compiling objects...\n",
      "  \u001b[31m   \u001b[0m Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  \u001b[31m   \u001b[0m ninja: error: '/tmp/pip-install-ruf20zbg/causal-conv1d_6adcc87d8b934d4ab1c1f004d6c48513/csrc/causal_conv1d.cpp', needed by '/tmp/pip-install-ruf20zbg/causal-conv1d_6adcc87d8b934d4ab1c1f004d6c48513/build/temp.linux-x86_64-cpython-310/csrc/causal_conv1d.o', missing and no known rule to make it\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ruf20zbg/causal-conv1d_6adcc87d8b934d4ab1c1f004d6c48513/setup.py\", line 309, in run\n",
      "  \u001b[31m   \u001b[0m     urllib.request.urlretrieve(wheel_url, wheel_filename)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/urllib/request.py\", line 241, in urlretrieve\n",
      "  \u001b[31m   \u001b[0m     with contextlib.closing(urlopen(url, data)) as fp:\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
      "  \u001b[31m   \u001b[0m     return opener.open(url, data, timeout)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/urllib/request.py\", line 525, in open\n",
      "  \u001b[31m   \u001b[0m     response = meth(req, response)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
      "  \u001b[31m   \u001b[0m     response = self.parent.error(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/urllib/request.py\", line 563, in error\n",
      "  \u001b[31m   \u001b[0m     return self._call_chain(*args)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
      "  \u001b[31m   \u001b[0m     result = func(*args)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/urllib/request.py\", line 643, in http_error_default\n",
      "  \u001b[31m   \u001b[0m     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "  \u001b[31m   \u001b[0m urllib.error.HTTPError: HTTP Error 404: Not Found\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m During handling of the above exception, another exception occurred:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2104, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     subprocess.run(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/subprocess.py\", line 526, in run\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, process.args,\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ruf20zbg/causal-conv1d_6adcc87d8b934d4ab1c1f004d6c48513/setup.py\", line 329, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/__init__.py\", line 117, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 183, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 199, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ruf20zbg/causal-conv1d_6adcc87d8b934d4ab1c1f004d6c48513/setup.py\", line 326, in run\n",
      "  \u001b[31m   \u001b[0m     super().run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/wheel/_bdist_wheel.py\", line 378, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/dist.py\", line 950, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/command/build_ext.py\", line 98, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 359, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 868, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     build_ext.build_extensions(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 476, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     self._build_extensions_serial()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 502, in _build_extensions_serial\n",
      "  \u001b[31m   \u001b[0m     self.build_extension(ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/command/build_ext.py\", line 263, in build_extension\n",
      "  \u001b[31m   \u001b[0m     _build_ext.build_extension(self, ext)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/setuptools/_distutils/command/build_ext.py\", line 557, in build_extension\n",
      "  \u001b[31m   \u001b[0m     objects = self.compiler.compile(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 681, in unix_wrap_ninja_compile\n",
      "  \u001b[31m   \u001b[0m     _write_ninja_file_and_compile_objects(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 1784, in _write_ninja_file_and_compile_objects\n",
      "  \u001b[31m   \u001b[0m     _run_ninja_build(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/envs/mamba-env/lib/python3.10/site-packages/torch/utils/cpp_extension.py\", line 2120, in _run_ninja_build\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(message) from e\n",
      "  \u001b[31m   \u001b[0m RuntimeError: Error compiling objects for extension\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for causal-conv1d\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (causal-conv1d)\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --no-cache-dir causal-conv1d>=1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
