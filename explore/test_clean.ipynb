{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mamba_ssm.modules.mamba_simple import Mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, length, dim = 2, 64, 16\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")\n",
    "model = Mamba(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=dim, # Model dimension d_model\n",
    "    d_state=16,  # SSM state expansion factor\n",
    "    d_conv=4,    # Local convolution width\n",
    "    expand=2,    # Block expansion factor\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0061, -0.0075, -0.0036,  ..., -0.0224, -0.0146,  0.0187],\n",
       "         [-0.0254,  0.0448, -0.0635,  ..., -0.0174,  0.0193,  0.0190],\n",
       "         [ 0.0478, -0.0245, -0.0183,  ...,  0.0371, -0.0228, -0.0039],\n",
       "         ...,\n",
       "         [-0.0108, -0.0210,  0.0096,  ...,  0.0085,  0.0064, -0.0039],\n",
       "         [ 0.0010,  0.0085,  0.0560,  ..., -0.0292, -0.0099, -0.0102],\n",
       "         [ 0.0068,  0.0129,  0.0102,  ...,  0.0255,  0.0638, -0.0435]],\n",
       "\n",
       "        [[-0.0515, -0.0012, -0.0054,  ..., -0.0310,  0.0122, -0.0046],\n",
       "         [ 0.0053,  0.0221,  0.0181,  ..., -0.0531, -0.0148,  0.0050],\n",
       "         [-0.0265,  0.0225, -0.0192,  ...,  0.0397,  0.0001, -0.0244],\n",
       "         ...,\n",
       "         [ 0.0538,  0.0222,  0.0367,  ..., -0.0277,  0.0373, -0.0108],\n",
       "         [ 0.0104, -0.0272,  0.0313,  ...,  0.0040,  0.0191, -0.0087],\n",
       "         [ 0.0245, -0.0115,  0.0113,  ..., -0.0025, -0.0172,  0.0341]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mamba(\n",
       "  (in_proj): Linear(in_features=16, out_features=64, bias=False)\n",
       "  (conv1d): Conv1d(32, 32, kernel_size=(4,), stride=(1,), padding=(3,), groups=32)\n",
       "  (act): SiLU()\n",
       "  (x_proj): Linear(in_features=32, out_features=33, bias=False)\n",
       "  (dt_proj): Linear(in_features=1, out_features=32, bias=True)\n",
       "  (out_proj): Linear(in_features=32, out_features=16, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Once upon a time?\\n\\nThe first time I saw the world,']\n"
     ]
    }
   ],
   "source": [
    "# load pretrained mamba model\n",
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "input_ids = tokenizer(\"Once upon a time?\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "out = model.generate(input_ids, max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey how are you doing?\\n\\nI'm so glad you're here.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_output(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    out = model.generate(input_ids, max_new_tokens=10)\n",
    "    return tokenizer.batch_decode(out)[0]\n",
    "\n",
    "get_output(\"Hey how are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1=2$ and $1+1=3$'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_output(\"1+1=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1=0$ and $1+1=1$.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "pythia_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\")\n",
    "pythia_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")\n",
    "\n",
    "def get_output_pythia(prompt):\n",
    "    input_ids = pythia_tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    out = pythia_model.generate(input_ids, max_new_tokens=10)\n",
    "    return tokenizer.batch_decode(out)[0]\n",
    "\n",
    "get_output_pythia(\"1+1=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with Mamba 130M...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text with Pythia-160M...\n",
      "\n",
      "Results:\n",
      "Mamba 130M generation time: 18.1766 seconds\n",
      "Pythia-160M generation time: 4.0884 seconds\n",
      "\n",
      "Mamba 130M generated text:\n",
      "Once upon a time, in a land far away, there lived a man who was a great hunter. He had a great hunting lodge, and he hunted for his prey. He hunted for his prey, and he killed many of them. He was, 196\n",
      "\n",
      "Pythia-160M generated text:\n",
      "Once upon a time, in a land far away, the\n",
      "\n",
      "government of the United States of America was established.\n",
      "\n",
      "The United States was a nation of the United States, and the\n",
      "\n",
      "United States was a nation of the United States, 213\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_length)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generation_time = end_time - start_time\n",
    "    \n",
    "    return generated_text, generation_time\n",
    "\n",
    "def main():\n",
    "    prompt = \"Once upon a time, in a land far away,\"\n",
    "    max_length = 50\n",
    "\n",
    "    # Mamba 130M model\n",
    "    mamba_model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "    mamba_tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "\n",
    "    # Pythia-160M model\n",
    "    pythia_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\")\n",
    "    pythia_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")\n",
    "\n",
    "    print(\"Generating text with Mamba 130M...\")\n",
    "    mamba_text, mamba_time = generate_text(mamba_model, mamba_tokenizer, prompt, max_length)\n",
    "    \n",
    "    print(\"Generating text with Pythia-160M...\")\n",
    "    pythia_text, pythia_time = generate_text(pythia_model, pythia_tokenizer, prompt, max_length)\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Mamba 130M generation time: {mamba_time:.4f} seconds\")\n",
    "    print(f\"Pythia-160M generation time: {pythia_time:.4f} seconds\")\n",
    "    print(f\"\\nMamba 130M generated text:\\n{mamba_text}, {len(mamba_text)}\")\n",
    "    print(f\"\\nPythia-160M generated text:\\n{pythia_text}, {len(pythia_text)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
